{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S183319 - Deep Learning - Week 1 - Hand-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise a)\n",
    "\n",
    "a) Write $y_j$ directly as a function of $x$. That is, eliminate the a´s and z´s:\n",
    "\n",
    "$$\n",
    "y_j = h_2(\\ldots)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer a)\n",
    "\n",
    "$$\n",
    "y_j = h_2 \\left( \\sum_{i=0}^M w^{(2)}_{ji} \\cdot h_1 \\left( \\sum_{i=0}^D w^{(1)}_{ji} x_i \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise b)\n",
    "\n",
    "b) Write the equation for a neural network with two hidden layers and three layers of weights $w=w^{(1)},w^{(2)},w^{(3)}$. Again, without using a´s and z´s.\n",
    "$$\n",
    "y_j = h_3(\\ldots)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer b)\n",
    "\n",
    "$$\n",
    "y_j = h_3 \\left( \\sum_{n=0}^N w^{(3)}_{jn} \\cdot h_2 \\left( \\sum_{m=0}^M w^{(2)}_{jm} \\cdot h_1 \\left( \\sum_{d=0}^D w^{(1)}_{jd} x_d \\right) \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise c)\n",
    "\n",
    "c) Write the equations for a FFNN with $L$ layers as recursion. Use $l$ as the index for the layer:\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j & = \\ldots & \\\\\n",
    "z^{(l)}_j & = h_l(\\ldots) & l=1,\\ldots,L \\\\\n",
    "a^{(l)}_j & = \\ldots &  l=2,\\ldots,L \\\\\n",
    "a^{(1)}_j & = \\ldots & \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer c)\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j & = z^{(L)}_j & \\\\\n",
    "z^{(l)}_j & = h_l(a^{(l)}_j) & l=1,\\ldots,L \\\\\n",
    "a^{(l)}_j & = \\sum_{i=0}^M w^{(l)}_{ji} z^{(l - 1)}_i &  l=2,\\ldots,L \\\\\n",
    "a^{(1)}_j & = \\sum_{i=0}^D w^{(1)}_{ji} x_i & \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise e)\n",
    "\n",
    "e) In this exercise you will show that with the two above assumptions we can derive a loss function that contains $E(w)$ as a term. Two hints\n",
    "\n",
    "1. With the used covariance we can write the Gaussian distribution as\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{t}_n|\\mathbf{y}(\\mathbf{x}_n),\\sigma^2 \\mathbf{I}) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )\n",
    "$$\n",
    "2. In order to turn maximum likelihood into a loss/error function apply the (natural) logarithm to the likelihood objective and multiply by minus one.\n",
    "\n",
    "Show that the loss we get is\n",
    "$$\n",
    "\\frac{ND}{2} \\log 2\\pi \\sigma^2 + \\frac{1}{2\\sigma^2} E(w) \\ . \n",
    "$$\n",
    "Further argue why applying the log and multiplying by minus one is the right thing to do in order to get a loss function. *Hint:* Will the optimum of the likelihood function change if we apply the logarithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer e)\n",
    "\n",
    "We know that,\n",
    "$$\n",
    "p(\\mathbf{t}_n|\\mathbf{x}_n,w) = \\mathcal{N}(\\mathbf{t}_n|\\mathbf{y}(\\mathbf{x}_n),\\sigma^2 \\mathbf{I}) \\ , \n",
    "$$\n",
    "$$\n",
    "p(\\mathbf{t}_1,\\ldots,\\mathbf{t}_N|\\mathbf{x}_1,\\ldots,\\mathbf{x}_N,w) =\n",
    "\\prod_{n=1}^N p(\\mathbf{t}_n|\\mathbf{x}_n,w) \\ .\n",
    "$$\n",
    "On the later we can apply the logarithm,\n",
    "$$\n",
    "log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) = \\sum_{n=1}^N \\log( p(\\mathbf{t}_n|\\mathbf{x}_n,w) )\n",
    "$$\n",
    "We can then substitute the prior on the right hand side,\n",
    "$$\n",
    "log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) = \\sum_{n=1}^N \\log( \\mathcal{N}(\\mathbf{t}_n|\\mathbf{y}(\\mathbf{x}_n),\\sigma^2 \\mathbf{I}) )\n",
    "$$\n",
    "Further substituting the Gaussian distribution,\n",
    "$$\n",
    "log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) = \\sum_{n=1}^N \\log( \\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ) ),\n",
    "$$\n",
    "Then actually resolving the logarithm,\n",
    "$$\n",
    "\\begin{align}\n",
    "    log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) &= \\sum_{n=1}^N (\\log( \\exp( - || \\mathbf{y}(\\mathbf{x}_n)) - \\mathbf{t}_n||_2^2 /2\\sigma^2 ) - \\log(\\sqrt{2\\pi \\sigma^2}^D)) \\\\\n",
    "    log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) &= \\sum_{n=1}^N (- \\frac{|| \\mathbf{y}(\\mathbf{x}_n)) - \\mathbf{t}_n||_2^2}{2\\sigma^2} - \\log(\\sqrt{2\\pi \\sigma^2}^D)) \\\\\n",
    "    log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) &= \\sum_{n=1}^N (- \\frac{|| \\mathbf{y}(\\mathbf{x}_n)) - \\mathbf{t}_n||_2^2}{2\\sigma^2} - \\frac{D}{2} \\log((2\\pi \\sigma^2))\\\\\n",
    "    log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) &= - \\frac{ND}{2} \\log((2\\pi \\sigma^2) + \\sum_{n=1}^N (- \\frac{|| \\mathbf{y}(\\mathbf{x}_n)) + \\mathbf{t}_n||_2^2}{2\\sigma^2})\n",
    "\\end{align}\n",
    "$$\n",
    "Then we multiply the equation by negative one, to create a minimization problem,\n",
    "$$\n",
    "\\begin{align}\n",
    "    - log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) &= \\frac{ND}{2} \\log((2\\pi \\sigma^2) + \\sum_{n=1}^N (\\frac{|| \\mathbf{y}(\\mathbf{x}_n)) - \\mathbf{t}_n||_2^2}{2\\sigma^2}) \\\\\n",
    "    - log(p(\\mathbf{t}_n|\\mathbf{x}_n,w)) &= \\frac{ND}{2} \\log((2\\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} E(w))\n",
    "\\end{align}\n",
    "$$\n",
    "As the logarithm is a monotone function, applying it to the maximum likelihood will not change any extrema. Then multiplying by negative one, turn the maximum likelihood into a minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise g) \n",
    "\n",
    "g) Show using the same procedure we used for regression that the loss function for classification is: \n",
    "$$\n",
    "E(w) = - \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{k}(\\mathbf{x}_n) \\ .\n",
    "$$\n",
    "This is also known as the cross entropy loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer g)\n",
    "\n",
    "Given that the data points are IID, we can write,\n",
    "$$\n",
    "p(\\mathbf{t}_n|\\mathbf{x}_n,w) = \\prod_{n=1}^N \\prod_{k=1}^K \\left[ y_k(\\mathbf{x}_n)\\right]^{t_{nk}} \\ .\n",
    "$$\n",
    "As previously, we can apply the logarithm to both sides,\n",
    "$$\n",
    "\\log( p(\\mathbf{t}_n|\\mathbf{x}_n,w) ) = \\log( \\prod_{n=1}^N \\prod_{k=1}^K \\left[ y_k(\\mathbf{x}_n)\\right]^{t_{nk}} ) \\ ,\n",
    "$$\n",
    "yielding,\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\log( p(\\mathbf{t}_n|\\mathbf{x}_n,w) ) &=  \\sum_{n=1}^N \\sum_{k=1}^K \\log ( \\left[ y_k(\\mathbf{x}_n)\\right]^{t_{nk}} ), \\\\\n",
    "    \\log( p(\\mathbf{t}_n|\\mathbf{x}_n,w) ) &=  \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log ( \\left[ y_k(\\mathbf{x}_n)\\right] ), \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Finally multiplying by negative one,\n",
    "$$\n",
    "- \\log( p(\\mathbf{t}_n|\\mathbf{x}_n,w) ) =  - \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log \\left(  y_k(\\mathbf{x}_n) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise i) \n",
    "Calculate \n",
    "$$\n",
    "\\delta^{(L)}_j = \\frac{\\partial E(w)}{\\partial a^{(L)}_{j}}\n",
    "$$\n",
    "for classification. \n",
    "\n",
    "Hint: It is much easier to find the derivative if we write the loss function directly in terms of the logits $a^{(L)}_{j}$. So show first using the definition of the loss and the softmax that \n",
    "$$\n",
    "E(w) = - \\sum_{k=1}^K t_k a^{(L)}_{k} + \\log \\sum_{k=1}^K \\exp( a^{(L)}_{k} ) \\ . \n",
    "$$\n",
    "Finally show that \n",
    "$$\n",
    "\\frac{\\partial}{\\partial a^{(L)}_{j}} \\log \\sum_{k=1}^K \\exp( a^{(L)}_{k} ) = y_j\n",
    "$$\n",
    "to get the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution i)\n",
    "\n",
    "Substituting softmax into the loss function,\n",
    "$$\n",
    "\\begin{align}\n",
    "    E(w) &= -  \\sum_{k=1}^K t_{nk} \\log(\\frac{\\exp(a_k^{(L)})}{\\sum_{j} \\exp(a_j^{(L)})}), \\\\ \n",
    "    E(w) &= -  \\sum_{k=1}^K t_{nk} \\log(\\exp(a_k^{(L)})) + \\sum_{k=1}^K t_{nk} \\log(\\sum_{j} \\exp(a_j^{(L)})), \\\\ \n",
    "    E(w) &= -  \\sum_{k=1}^K t_{nk} a_k^{(L)} + \\sum_{k=1}^K t_{nk} \\log(\\sum_{j} \\exp(a_j^{(L)})). \\\\  \n",
    "\\end{align}\n",
    "$$\n",
    "Using a logarithm rule, somewhat in reverse,\n",
    "$$\n",
    "E(w) = - \\sum_{k=1}^K t_{k} a_k^{(L)} + \\sum_{k=1}^K \\log((\\sum_{j=1}^J \\exp(a_j^{(L)}))^{ t_{nk}}) \\\\ \n",
    "$$\n",
    "Since the t-vector is zero in all cases but one, the expression will be raised to the power of zero and hence assume a value of one, since the logarith of one is zero, we only have to consider one case,\n",
    "$$\n",
    "E(w) = - \\sum_{k=1}^K t_{k} a_k^{(L)} + \\log((\\sum_{k=1}^K \\exp(a_k^{(L)}))) \\\\ \n",
    "$$\n",
    "We then differentiate the above expression with respect to $a_j$. The first sum, just becomes $-t_j$, as all but one indice of t is zero and the final one is (numeric) one, then it is simply $\\frac{-t_j a_j}{a_j} = -t_j$.\n",
    "On the second sum, we need to utilize the chain-rule of differentiation,\n",
    "$$\n",
    "\\frac{d}{dx} f ( g (x) ) = f' (g (x) ) \\cdot g' (x).\n",
    "$$\n",
    "This yields,\n",
    "$$\n",
    "\\delta_j^{(L)} = \\frac{\\partial E(w)}{\\partial a_j^{(L)}} = \\frac{1}{\\sum_{j=1}^J \\exp(a_j^{(L)})} \\cdot \\exp(a_j^{(L)}) = \\frac{\\exp(a_j^{(L)})}{\\sum_{j=1}^J \\exp(a_j^{(L)})}\n",
    "$$\n",
    "Combining the two produces the final result,\n",
    "$$\n",
    "\\delta_j^{(L)} = -t_j + \\frac{\\exp(a_j^{(L)})}{\\sum_{j=1}^J \\exp(a_j^{(L)})} = -t_j + y_j\n",
    "$$\n",
    "We can actually further simplify, by realsing that $t_j$, must assume the value $1$, since for any $t_k = 0$, where $k \\neq j$.\n",
    "$$\n",
    "\\delta_j^{(L)} = y_j - 1.\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise j) - the backpropagation rule for layer $l<L$\n",
    "\n",
    "j) Use the above results to argue why the general backpropagation rule for $l<L$ is written as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E(w)}{\\partial w^{(l)}_{ji}} & = \\delta^{(l)}_j  z^{(l-1)}_i \\\\\n",
    "\\delta^{(l)}_j & = \\sum_{k=1}^K \\delta^{(l+1)}_k  w^{(l+1)}_{kj} h_{l}'(a^{(l)}_j) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution j)\n",
    "\n",
    "By using the chain rule - in the general case we get,\n",
    "$$\n",
    "    \\frac{ \\partial E(w) }{ \\partial w_{ji}^{(l)} } \n",
    "    =\n",
    "    \\sum_{k = 1}^{N} \\sum_{m=1}^{M} \\cdots \\sum_{k = 1}^{K}\n",
    "    \\frac{ \\partial E(w) }{ \\partial a_{n}^{(L)} }\n",
    "    \\frac{ \\partial a_{n}^{(L)} }{ \\partial a_{m}^{(L - 1)} }\n",
    "    \\cdots\n",
    "    \\frac{ \\partial a_{k}^{(l + 1)} }{ \\partial a_{j}^{(l)} }\n",
    "    \\frac{ \\partial a_{j}^{(l)} }{ \\partial w_{ji}^{(l)} }\n",
    "$$\n",
    "We then substitute $ \\frac{ \\partial E(w) }{ \\partial a_{n}^{(L)} } $ for $\\delta_n^{(L )} $,\n",
    "$$\n",
    "    \\frac{ \\partial E(w) }{ \\partial w_{ji}^{(l)} } \n",
    "    =\n",
    "    \\sum_{k = 1}^{N} \\sum_{m=1}^{M} \\cdots \\sum_{k = 1}^{K}\n",
    "    \\delta_n^{(L )}\n",
    "    \\frac{ \\partial a_{n}^{(L)} }{ \\partial a_{m}^{(L - 1)} }\n",
    "    \\cdots\n",
    "    \\frac{ \\partial a_{k}^{(l + 1)} }{ \\partial a_{j}^{(l)} }\n",
    "    \\frac{ \\partial a_{j}^{(l)} }{ \\partial w_{ji}^{(l)} }\n",
    "$$\n",
    "Additionally we substitute $ \\partial a_{n}^{(L)} $ for $ \\sum_i w^{(L)}_{ni} h_{L-1}(a^{(L-1)}_i) $ (as previously defined),\n",
    "$$\n",
    "    \\frac{ \\partial E(w) }{ \\partial w_{ji}^{(l)} } \n",
    "    =\n",
    "    \\sum_{k = 1}^{N} \\sum_{m=1}^{M} \\cdots \\sum_{k = 1}^{K}\n",
    "    \\delta_n^{(L )}\n",
    "    \\frac{ \\sum_i w^{(L)}_{ni} h_{L-1}(a^{(L-1)}_i) }{ \\partial a_{m}^{(L - 1)} }\n",
    "    \\cdots\n",
    "    \\frac{ \\partial a_{k}^{(l + 1)} }{ \\partial a_{j}^{(l)} }\n",
    "    \\frac{ \\partial a_{j}^{(l)} }{ \\partial w_{ji}^{(l)} }\n",
    "$$\n",
    "This derivative only yields something when i assumes the value m, in which case the sum over i can be neglected, and the devirative yields,\n",
    "$$\n",
    "    \\frac{ \\partial E(w) }{ \\partial w_{ji}^{(l)} } \n",
    "    =\n",
    "    \\sum_{k = 1}^{N} \\sum_{m=1}^{M} \\cdots \\sum_{k = 1}^{K}\n",
    "    \\delta_n^{(L )}\n",
    "    w^{(L)}_{nm} h_{L-1} ' (a^{(L-1)}_m)\n",
    "    \\cdots\n",
    "    \\frac{ \\partial a_{k}^{(l + 1)} }{ \\partial a_{j}^{(l)} }\n",
    "    \\frac{ \\partial a_{j}^{(l)} }{ \\partial w_{ji}^{(l)} }\n",
    "$$\n",
    "We can then use the expression for $ \\delta^{(L-1)}_j = \\sum_{k=1}^K \\delta^{(L)}_k  w^{(L)}_{kj} h_{L-1}'(a^{(L-1)}_j) $, substituting yields,\n",
    "$$\n",
    "    \\frac{ \\partial E(w) }{ \\partial w_{ji}^{(l)} } \n",
    "    =\n",
    "    \\sum_{m=1}^{M} \\cdots \\sum_{k = 1}^{K}\n",
    "    \\delta^{(L-1)}_m\n",
    "    \\cdots\n",
    "    \\frac{ \\partial a_{k}^{(l + 1)} }{ \\partial a_{j}^{(l)} }\n",
    "    \\frac{ \\partial a_{j}^{(l)} }{ \\partial w_{ji}^{(l)} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, by substituting we have eliminated one of the $L - l$ sums, which ultimately results in going down a layer of $\\delta$. So we see that we can general substitution,\n",
    "$$\n",
    "    \\delta^{(l)}_j = \\sum_{k=1}^K \\delta^{(l + 1)}_k  w^{(l + 1)}_{kj} h_{l}'(a^{(l)}_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in our equation for $ \\frac{ \\partial E(w) }{ \\partial w_{ji}^{(l)} }$ the pattern becomes,\n",
    "$$\n",
    "   \\begin{align}\n",
    "      \\frac{ \\partial E(w) }{ \\partial w_{ji}^{(l)} }\n",
    "      &=\n",
    "      \\sum_{m=1}^{M}\n",
    "      \\delta^{(l + 1)}_m w_{mj}^{(l + 1)} h_l'(a_j^{(l)}) \\frac{\\partial a_j^{(l)}}{\\partial w_ji^{(l)}}\n",
    "      \\\\\n",
    "      &= \\delta_j^{(l)} z_i^{(l - 1)}\n",
    "   \\end{align}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise l)\n",
    "l) Modify the backpropagation rules to be able to take a dataset of size greater than one. \n",
    "Hint: This only requires introducing a sum over $n$ and $n$ indices in a few places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution l)\n",
    "To handle datasets of size larger than one, we simply introduce the notation of n-indices. The n-indices can be written behind the variable, as:  ${}^{(n)}\\delta_j^{(l)}$.\n",
    "\n",
    "Utilizing this on our general backpropagation rule (for $l < L$), yields:\n",
    "$$\n",
    "    \\frac{ \\partial E(w) }{ \\partial w_{ji}^{(l)} } = \\sum_{n=1}^{N} {}^{(n)}\\delta_j^{(l)} \\cdot {}^{(n)}z_i^{(l - 1)},\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "    {}^{(n)}\\delta^{(l)}_j = \\sum_{k=1}^K {}^{(n)}\\delta^{(l+1)}_k  w^{(l+1)}_{kj} h_{l}'({}^{(n)}a^{(l)}_j)\n",
    "$$\n",
    "When introducing the n-indices we must make certain to sum over all of them, hence the sum over n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d96a798051220adb8d47ede7819712d4980d7e1ecee887457e300fc8d0177c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
